{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9ce5c1-6977-4cfd-88ae-5a970c7874e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a989c37f-86a8-4f99-b41b-257f51c1dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcb7474-44a8-4ab9-9da6-7f2fcd1f90a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clim-lab/miniconda3/envs/MEaI_transformer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-28 18:21:08.316984: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 18:21:08.480098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-28 18:21:08.946637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/clim-lab/miniconda3/envs/MEaI_transformer/lib/\n",
      "2022-11-28 18:21:08.946739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/clim-lab/miniconda3/envs/MEaI_transformer/lib/\n",
      "2022-11-28 18:21:08.946746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ReformerConfig, ReformerModel\n",
    "import torch\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d6f4ca-546d-4f0d-859a-3d7089b2c107",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): ReformerLayer(\n",
       "      (attention): ReformerAttention(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (self_attention): LocalSelfAttention(\n",
       "          (query): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (key): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=768, bias=False)\n",
       "        )\n",
       "        (output): ReformerSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): ChunkReformerFeedForward(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (dense): ReformerFeedForwardDense(\n",
       "          (act_fn): ReLU()\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): ReformerFeedForwardOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ReformerLayer(\n",
       "      (attention): ReformerAttention(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (self_attention): LSHSelfAttention(\n",
       "          (query_key): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=768, bias=False)\n",
       "        )\n",
       "        (output): ReformerSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): ChunkReformerFeedForward(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (dense): ReformerFeedForwardDense(\n",
       "          (act_fn): ReLU()\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): ReformerFeedForwardOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ReformerLayer(\n",
       "      (attention): ReformerAttention(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (self_attention): LocalSelfAttention(\n",
       "          (query): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (key): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=768, bias=False)\n",
       "        )\n",
       "        (output): ReformerSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): ChunkReformerFeedForward(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (dense): ReformerFeedForwardDense(\n",
       "          (act_fn): ReLU()\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): ReformerFeedForwardOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ReformerLayer(\n",
       "      (attention): ReformerAttention(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (self_attention): LSHSelfAttention(\n",
       "          (query_key): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=768, bias=False)\n",
       "        )\n",
       "        (output): ReformerSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): ChunkReformerFeedForward(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (dense): ReformerFeedForwardDense(\n",
       "          (act_fn): ReLU()\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): ReformerFeedForwardOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): ReformerLayer(\n",
       "      (attention): ReformerAttention(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (self_attention): LocalSelfAttention(\n",
       "          (query): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (key): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=768, bias=False)\n",
       "        )\n",
       "        (output): ReformerSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): ChunkReformerFeedForward(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (dense): ReformerFeedForwardDense(\n",
       "          (act_fn): ReLU()\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): ReformerFeedForwardOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): ReformerLayer(\n",
       "      (attention): ReformerAttention(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (self_attention): LSHSelfAttention(\n",
       "          (query_key): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=256, out_features=768, bias=False)\n",
       "        )\n",
       "        (output): ReformerSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): ChunkReformerFeedForward(\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "        (dense): ReformerFeedForwardDense(\n",
       "          (act_fn): ReLU()\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (output): ReformerFeedForwardOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 30\n",
    "\n",
    "reformer_config = ReformerConfig(vocab_size=WINDOW_SIZE)\n",
    "reformer_full = ReformerModel(reformer_config)\n",
    "\n",
    "for i,layer in enumerate(reformer_full.children()):\n",
    "    if i == 0:\n",
    "        reformer_embedding = layer\n",
    "    else:\n",
    "        reformer_encoder = layer\n",
    "reformer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de39240-1d61-4637-af7c-0241349c0afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerConfig {\n",
       "  \"attention_head_size\": 64,\n",
       "  \"attn_layers\": [\n",
       "    \"local\",\n",
       "    \"lsh\",\n",
       "    \"local\",\n",
       "    \"lsh\",\n",
       "    \"local\",\n",
       "    \"lsh\"\n",
       "  ],\n",
       "  \"axial_norm_std\": 1.0,\n",
       "  \"axial_pos_embds\": true,\n",
       "  \"axial_pos_embds_dim\": [\n",
       "    64,\n",
       "    192\n",
       "  ],\n",
       "  \"axial_pos_shape\": [\n",
       "    64,\n",
       "    64\n",
       "  ],\n",
       "  \"chunk_size_lm_head\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"feed_forward_size\": 512,\n",
       "  \"hash_seed\": null,\n",
       "  \"hidden_act\": \"relu\",\n",
       "  \"hidden_dropout_prob\": 0.05,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"local_attention_probs_dropout_prob\": 0.05,\n",
       "  \"local_attn_chunk_length\": 64,\n",
       "  \"local_num_chunks_after\": 0,\n",
       "  \"local_num_chunks_before\": 1,\n",
       "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
       "  \"lsh_attn_chunk_length\": 64,\n",
       "  \"lsh_num_chunks_after\": 0,\n",
       "  \"lsh_num_chunks_before\": 1,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"reformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_buckets\": null,\n",
       "  \"num_hashes\": 1,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fa47d4-2290-417b-9227-977bb9c65390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class compare_former_model(nn.Module):\n",
    "    def __init__(self,encoder,encoder_config):\n",
    "        super(compare_former_model, self).__init__()\n",
    "        self.d_model = encoder_config.hidden_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = nn.Linear(self.d_model,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        logits = self.decoder(encoder_out.last_hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1847fa37-e875-4ffd-8b25-40ee31cd00b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m30\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m256\u001b[39m])\n\u001b[1;32m      2\u001b[0m tmp_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tmp)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mreformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MEaI_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/MEaI_transformer/lib/python3.9/site-packages/transformers/models/reformer/modeling_reformer.py:1733\u001b[0m, in \u001b[0;36mReformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, num_hashes, past_buckets_states, use_cache, orig_sequence_length, output_hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# concat same tensor for reversible ResNet\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([hidden_states, hidden_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1733\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43m_ReversibleFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hashes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_buckets_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;66;03m# Apply layer norm to concatenated hidden states\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/MEaI_transformer/lib/python3.9/site-packages/transformers/models/reformer/modeling_reformer.py:1617\u001b[0m, in \u001b[0;36m_ReversibleFunction.forward\u001b[0;34m(ctx, hidden_states, layers, attention_mask, head_mask, num_hashes, all_hidden_states, all_attentions, past_buckets_states, use_cache, orig_sequence_length, output_hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;66;03m# split duplicated tensor\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m hidden_states, attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(hidden_states, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1617\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id, (layer, layer_head_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m         all_hidden_states\u001b[38;5;241m.\u001b[39mappend(hidden_states)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "tmp = np.random.rand(30*256).reshape([1,30,256])\n",
    "tmp_input = torch.tensor(tmp)\n",
    "\n",
    "reformer_encoder(tmp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a60448-320b-4cba-af88-cd62484ec143",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([[1,2,3,45,6]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
